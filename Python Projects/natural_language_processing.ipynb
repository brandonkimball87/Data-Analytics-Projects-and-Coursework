{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Section 1"
      ],
      "metadata": {
        "id": "L7IHhsFvoEij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the folder and file paths \n",
        "# Adjust this based on the file paths each time code is ran\n",
        "capability_folder = '/content/drive/MyDrive/Kidney_documents'\n",
        "requirement_csv_file = '/content/drive/MyDrive/Kidney_req.csv'\n",
        "output_folder = \"/content/drive/MyDrive/Kidney_output/\"\n",
        "output_file = \"/content/drive/MyDrive/Kidney_output/Comparison_Matrix.csv\"\n",
        "\n",
        "\n",
        "# Indicating the number of capability paragraphs per requirement\n",
        "capabilities_per_req = 5\n",
        "\n",
        "\n",
        "# Editing the list of kidney specific stop words\n",
        "kidney_custom = ['Kidney','Renal','Nephron','Glomerulus','Urea']"
      ],
      "metadata": {
        "id": "yJwbTMcpgtNM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Section 2"
      ],
      "metadata": {
        "id": "8ZbzCP6fo6Vb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVrhwAvwF4EG"
      },
      "outputs": [],
      "source": [
        "pip install -q textract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kSq2g575L0A_"
      },
      "outputs": [],
      "source": [
        "pip install -q gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import gensim\n",
        "import csv\n",
        "import textract \n",
        "import spacy\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"stopwords\")\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "0g3ESRjZrE53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The pandas version is: \",pd.__version__)\n",
        "print(\"The numpy version is: \",np.__version__)\n",
        "print(\"The os version is: \", os.uname().release)\n",
        "print(\"The csv version is: \",csv.__version__)\n",
        "print(\"The textract version is: \")\n",
        "!pip show textract | grep Version\n",
        "print(\"The spacy version is: \",spacy.__version__)\n",
        "print(\"The gensim version is: \", gensim.__version__)\n",
        "print(\"Doc2Vec and TaggedDocument is from gensim\")\n",
        "print(\"The nltk version is:\", nltk.__version__)\n",
        "print(\"stopwords and word_tokenize is from nltk\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjs0ygMUj9c1",
        "outputId": "ed823c3b-c5b3-43a9-d275-b716739497c9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The pandas version is:  1.5.3\n",
            "The numpy version is:  1.22.4\n",
            "The os version is:  5.10.147+\n",
            "The csv version is:  1.0\n",
            "The textract version is: \n",
            "Version: 1.6.5\n",
            "The spacy version is:  3.5.2\n",
            "The gensim version is:  4.3.1\n",
            "Doc2Vec and TaggedDocument is from gensim\n",
            "The nltk version is: 3.8.1\n",
            "stopwords and word_tokenize is from nltk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTurpzPSHji4",
        "outputId": "850e273f-0918-430a-9c6d-3d75d6af4320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mounting the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Section 3"
      ],
      "metadata": {
        "id": "B5bn35NNr4B2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fHOFDnYwuYO8"
      },
      "outputs": [],
      "source": [
        "# importing the capability file names\n",
        "folder_path = capability_folder\n",
        "capability_names = []\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.docx'):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        text = textract.process(file_path, extension='docx')\n",
        "        text = text.decode(\"utf-8\")\n",
        "        text = text.replace(\"/\",\"\")\n",
        "        text = text.replace(\"\\t\",\"\")\n",
        "        capability_names.append(filename)\n",
        "    elif filename.endswith('.pdf'):\n",
        "        file_path = os.path.join(folder_path,filename)\n",
        "        pdftext = textract.process(file_path, method='pdfminer')\n",
        "        pdftext = pdftext.decode(\"utf-8\")\n",
        "        pdftext = pdftext.replace(\"/\",\"\")\n",
        "        pdftext = pdftext.replace(\"\\t\",\"\")\n",
        "        capability_names.append(filename)\n",
        "    \n",
        "# Adding the capability names to the first column of a capability array \n",
        "capabilities = np.empty((len(capability_names), 3), dtype='object')\n",
        "capabilities[:, 0] = capability_names\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Importing the capability text\n",
        "folder_path = capability_folder\n",
        "imported_capabilities = []\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.docx'):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        # Do something with the PDF file\n",
        "        text = textract.process(file_path, extension='docx')\n",
        "        text = text.decode(\"utf-8\")\n",
        "        text = text.replace(\"/\",\"\")\n",
        "        text = text.replace(\"\\t\",\"\")\n",
        "        imported_capabilities.append(text)\n",
        "    else:\n",
        "        file_path = os.path.join(folder_path,filename)\n",
        "        pdftext = textract.process(file_path, method='pdfminer')\n",
        "        pdftext = pdftext.decode(\"utf-8\")\n",
        "        pdftext = pdftext.replace(\"/\",\"\")\n",
        "        pdftext = pdftext.replace(\"\\t\",\"\")\n",
        "        imported_capabilities.append(pdftext)\n",
        "\n",
        "# Adding the full document capability text to the 2nd column of the capability array \n",
        "capabilities[:, 1] = imported_capabilities\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Splitting the 2nd column documents into individual paragraphs and pasting it to 2nd and 3rd column\n",
        "capability_paragraph = []\n",
        "for row in capabilities:\n",
        "    split_values = row[1].split(\"\\n\\n\\n\\n\")\n",
        "    for value in split_values:\n",
        "        new_row = [row[0], value, value]\n",
        "        capability_paragraph.append(new_row)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Deleting all rows which have empty values\n",
        "array = np.array(capability_paragraph)\n",
        "indices = np.where(array[:, 2] == \"\")[0]\n",
        "capability = np.delete(array, indices, axis=0)\n",
        "\n",
        "# Deleting all rows which have \"paragraphs\" less than 12 words\n",
        "array = np.array(capability)\n",
        "min_words = 12    # change this is you want a number other than 12\n",
        "indices = []\n",
        "for i, row in enumerate(array):\n",
        "    num_words = len(row[2].split())\n",
        "    if num_words < min_words:\n",
        "        indices.append(i)\n",
        "capability = np.delete(array, indices, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Section 4"
      ],
      "metadata": {
        "id": "ZCNq1fZTtKWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Stop words\n",
        "en = spacy.load('en_core_web_sm')\n",
        "sw_spacy = en.Defaults.stop_words \n",
        "stopwords = sw_spacy.union(kidney_custom)\n",
        "\n",
        "# Removing the stop words from the second column of the capability array\n",
        "for i, row in enumerate(capability):\n",
        "  doc_tokens = [token.lower() for token in row[2].split() if token.lower() not in stopwords]\n",
        "  nostopwords = \" \".join(doc_tokens)\n",
        "  capability[i][2] = nostopwords"
      ],
      "metadata": {
        "id": "eRuBs4ORFgAk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Section 5"
      ],
      "metadata": {
        "id": "sUvb_1FZt_tH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the csv file which contains the broken down requirement document into paragraphs\n",
        "file_path = requirement_csv_file\n",
        "\n",
        "os.path.exists(file_path)\n",
        "df = pd.read_csv(file_path, header=None, names=['Requirement_Paragraph'])\n",
        "requirement_csv = df.values.tolist()"
      ],
      "metadata": {
        "id": "sCPBq4lvQBdW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataframe which will contain the requirement paragraph and the corresponding score and capability\n",
        "Final = pd.DataFrame(requirement_csv, columns = ['Requirement_Paragraph'])"
      ],
      "metadata": {
        "id": "AlKndcopQ0FC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the stop words from the requirement paragraphs to be used in Doc2Vec \n",
        "req_nostop = []\n",
        "for req in requirement_csv:\n",
        "    req_str = str(req[0])\n",
        "    req_tokens = [token.lower() for token in req_str.split() if token.lower() not in stopwords]\n",
        "    req_nostop.append(' '.join(req_tokens))"
      ],
      "metadata": {
        "id": "pxzU5M5vJ8IW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Section 6"
      ],
      "metadata": {
        "id": "gfi8D_GKuCmN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0zKBZXIEVShz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "486b5ba9-2bcf-46bf-aea1-ec5267b74e92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
            "<ipython-input-18-ce481b5eeaa5>:24: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
            "  similar_doc = model.docvecs.most_similar([vector], topn = number) # Finds the document(s) that has the most similar vector representation to \"vector\" based on cosine simularity (and also stores the cosine simularity score)\n",
            "<ipython-input-18-ce481b5eeaa5>:24: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
            "  similar_doc = model.docvecs.most_similar([vector], topn = number) # Finds the document(s) that has the most similar vector representation to \"vector\" based on cosine simularity (and also stores the cosine simularity score)\n"
          ]
        }
      ],
      "source": [
        "# Building the doc2vec model\n",
        "tagged_data = [TaggedDocument(words=word_tokenize(_d), tags=[str(i)]) for i, _d in enumerate(capability[:,2])]\n",
        "model = Doc2Vec(vector_size = 50, alpha = 0.025, min_alpha = 0.00025, min_count=1, dm =1)\n",
        "max_epochs = 5   # The number of times it will run\n",
        "model.build_vocab(tagged_data)\n",
        "for epoch in range(max_epochs):\n",
        "    #print('iteration {0}'.format(epoch))\n",
        "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "    model.alpha -= 0.0002 # decrease the learning rate\n",
        "    model.min_alpha = model.alpha # fix the learning rate, no decay \n",
        "\n",
        "# Saving and loading the doc2vec model \n",
        "#model.save(\"d2v.model\")\n",
        "#print(\"Model Saved\")\n",
        "#model= Doc2Vec.load(\"d2v.model\")\n",
        "\n",
        "number = capabilities_per_req    # The number of capability examples you want, defined at the begining of the code\n",
        "\n",
        "# Running the model\n",
        "for i in range(len(req_nostop)):\n",
        "    test_data = word_tokenize(req_nostop[i])   # Tokenizes the text in the req_nostop list\n",
        "    vector = model.infer_vector(test_data) # Generate a vector representation of the document\n",
        "\n",
        "    similar_doc = model.docvecs.most_similar([vector], topn = number) # Finds the document(s) that has the most similar vector representation to \"vector\" based on cosine simularity (and also stores the cosine simularity score)\n",
        "\n",
        "    for j in range(number):\n",
        "      Final.loc[i, (j*3)+1] = capability[int(similar_doc[j][0])][1]\n",
        "      Final.loc[i, (j*3)+2] = similar_doc[j][1]\n",
        "      Final.loc[i, (j*3)+3] = capability[int(similar_doc[j][0])][0]\n",
        "\n",
        "\n",
        "#print(model.docvecs['1']) # the vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Section 7"
      ],
      "metadata": {
        "id": "ZU1Wn0tUuHE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Edit the name of the column headers\n",
        "new_cols = []\n",
        "for i in range(1, capabilities_per_req+1):\n",
        "    new_cols.extend(['capability_{}'.format(i), 'score_{}'.format(i), 'file_name_{}'.format(i)])\n",
        "Final.rename(columns=dict(zip(Final.columns[1:], new_cols)), inplace=True)"
      ],
      "metadata": {
        "id": "ciGyT3sRLLGq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6coI0Ng-weR8"
      },
      "outputs": [],
      "source": [
        "# Exporting the dataframe to the google drive\n",
        "# The first line is creating the directory folder (defined at the beginning)\n",
        "# The second line is where you can edit the name of the file (defined at the beginning)\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "Final.to_csv(output_file)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
